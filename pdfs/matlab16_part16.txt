这里xk是当前权重和偏置向量，g k是当前梯度，a k是学习速率。有两种不同的办法实
现梯度下降算法：增加模式和批处理模式。在增加模式中，网络输入每提交一次，梯度计算
一次并更新权重。在批处理模式中，当所有的输入都被提交后网络才被更新。下面两节将讨
论增加模式和批处理模式。 
  增加模式训练法（ADAPT） 
  函数adapt用来训练增加模式的网络，它从训练设置中接受网络对象、网络输入和目标
输入，返回训练过的网络对象、用最后的权重和偏置得到的输出和误差。 
  这里有几个网络参数必须被设置，第一个是net.adaptFcn，它决定使用哪一种增加模式
函数，缺省值为adaptwb，这个值允许每一个权重和偏置都指定它自己的函数，这些单个的
学
习
函
数
由
参
数
net.biases{i,j}.learnFcn
、
net.inputWeights{i,j}.learnFcn
、 
net.layerWeights{i,j}.learnFcn和Gradient Descent (LEARDGD)来决定。
对于基本的梯度最速下
降算法，权重和偏置沿着性能函数的梯度的负方向移动。在这种算法中，单个的权重和偏置
的学习函数设定为"learngd"。下面的命令演示了怎样设置前面建立的前馈函数参数： 
  net.biases{1,1}.learnFcn = 'learngd'; 
  net.biases{2,1}.learnFcn = 'learngd'; 
  net.layerWeights{2,1}.learnFcn = 'learngd'; 
  net.inputWeights{1,1}.learnFcn = 'learngd'; 
  函数learngd有一个相关的参数--学习速率lr。权重和偏置的变化通过梯度的负数乘上学
习速率倍数得到。学习速率越大，步进越大。如果学习速率太大算法就会变得不稳定。如果
学习速率太小，算法就需要很长的时间才能收敛。当learnFcn设置为learngd时，就为每一个
权重和偏置设置了学习速率参数的缺省值，
如上面的代码所示，
当然你也可以自己按照意愿
改变它。下面的代码演示了把层权重的学习速率设置为0.2。我们也可以为权重和偏置单独
的设置学习速率。 
  net.layerWeights{2,1}.learnParam