个元素都通过权重矩阵W和每一个神经元连接起
来。第I个神经元通过把所有加权的输入和偏置加起来得到它自己的标量输出n(i)。不同的
n(i)合起来形成了有S个元素的网络输入向量n。最后，网络层输出一个列向量a，我们在图
的底部显示了a的表达式。 
  注意输入元素个数R和神经元个数S通常是不等的，
我们也并不需要这两者相等。
你也可
以建立一个简单的复合神经元层，
它将上面所示的网络并行的合在一起，
使用不同的转移函
数。所有的网络都有相同的输入，而每一个网络都会产生输出。 
  输入向量元素经加权矩阵W作用输入网络。 
    W= 
 
  注意加权矩阵W的行标标记权重的目的神经元，列标标记待加权的输入标号。因此， 的
标号表示从输入信号的第二个元素到第一个神经元的权重是 。有S个神经元和R个输入元素
的神经网络也能够简化成以下符号： 
 
  
  
  这里，p是一个有R个元素的输入向量，W是一个SxR的矩阵，a和b是有S个元素的向量。
如前面所定义的，神经元层包括权重矩阵，乘法运算，偏置向量b，求和符和转移函数框。 
  输入和层 
  我们将要讨论多层网络，
所以我们需要拓展我们的符号来描述这样的网络。
特别是我们
要弄清连接输入的权重矩阵和连接层的权重矩阵之间的区别。
我们也要分清权重矩阵的目的
和源。 
  我们将把连接输入的权重矩阵成为输入权重，
把来自层输出的权重矩阵称为层矩阵。
进
一步说，我们在各个权重和其他网络元素中将用上标区分源（第二个标号）和目的（第一个
标号）。作为示例，我们用简化的形式重画了上面所画的单层多输入网络。 
  你可以看到，
我们把连接输入向量p的权重矩阵标记为输入权重矩阵(IW1,1)，
第二个标
号1 是源，第二个标号1 是目的。同样，第一层的元素，比如偏置、网络输入和输出都有上
标1 来表示它们属于第一层。 
  在下一章节，我们将用LW表示层权重矩阵，用IW表示输入权重矩阵。 
  你可以复习以下这一章开始的符号那一节，它把特定的网络net中用数学符号表示的层
权重矩阵转换成代码，如下所示： 
  IW1,1 net. IW{1,1} 
  这样,你就可以写代码来得到对转移函数的网络输入了： 
  n{1}=net.IW{1,1}*p+net.b{1}
多层神经元网络 
  一个网络可以有几层，每一层都有权重矩阵W