三个神经元，第二层有一个神经元。第一层的转
移函数是tan-sigmoid，输出层的转移函数是linear。输入向量的第一个元素的范围是-1 到2，
输入向量的第二个元素的范围是0 到5，训练函数是traingd。 
  net=newff([-1 2; 0 5],[3,1],{'tansig','purelin'},'traingd'); 
  这个命令建立了网络对象并且初始化了网络权重和偏置，因此网络就可以进行训练了。
我们可能要多次重新初始化权重或者进行自定义的初始化。下面就是初始化的详细步骤。 
  在训练前馈网络之前，权重和偏置必须被初始化。初始化权重和偏置的工作用命令init
来实现。
这个函数接收网络对象并初始化权重和偏置后返回网络对象。
下面就是网络如何初
始化的： 
  net = init(net); 
  我们可以通过设定网络参数net.initFcn和net.layer{i}.initFcn这一技巧来初始化一个给定
的网络。net. initFcn用来决定整个网络的初始化函数。前馈网络的缺省值为initlay，它允许
每一层用单独的初始化函数。设定了net.initFcn ，那么参数net.layer{i}.initFcn 也要设定用
来决定每一层的初始化函数。 
  对前馈网络来说，有两种不同的初始化方式经常被用到：initwb和initnw。initwb函数根
据每一层自己的初始化参数(net.inputWeights{i,j}.initFcn)初始化权重矩阵和偏置。前馈网络
的初始化权重通常设为rands，它使权重在-1 到1 之间随机取值。这种方式经常用在转换函
数是线性函数时。initnw通常用于转换函数是曲线函数。它根据Nguyen和Widrow[NgWi90]
为层产生初始权重和偏置值，
使得每层神经元的活动区域能大致平坦的分布在输入空间。
它
比起单纯的给权重和偏置随机赋值有以下优点：
（1）减少神经元的浪费（因为所有神经元的
活动区域都在输入空间内）
。
（2）有更快的训练速度（因为输入空间的每个区域都在活动的
神经元范围中）
。 
  初始化函数被newff所调用。因此当网络创建时，它根据缺省的参数自动初始化。init不
需要单独的调用。可是我们可能要重新初始化权重和偏置或者进行自定义的初始化。例如，
我们用newff创建